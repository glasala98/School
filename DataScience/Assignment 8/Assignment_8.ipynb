{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'mean_absolute_percentage_error' from 'sklearn' (C:\\Users\\Gerrid\\anaconda3\\lib\\site-packages\\sklearn\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-3c7478dfc844>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mXGBRegressor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmean_absolute_percentage_error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'mean_absolute_percentage_error' from 'sklearn' (C:\\Users\\Gerrid\\anaconda3\\lib\\site-packages\\sklearn\\__init__.py)"
     ]
    }
   ],
   "source": [
    "# You can import *ANYTHING* you want here.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import mean_absolute_percentage_error\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 8: Tree methods\n",
    "\n",
    "In this assignment we'll study tree methods and their capabilities as interpolators and extrapolators, and the importance of understanding your data. We will use a dataset of energy consumption of home appliances for houses in Chievres, Belgium. The data has the following variables:\n",
    "\n",
    "- date time year-month-day hour:minute:second\n",
    "- Appliances, energy use in Wh (**Target variable**)energy_extrapolation\n",
    "- lights, energy use of light fixtures in the house in Wh\n",
    "- T1, Temperature in kitchen area, in Celsius\n",
    "- RH_1, Humidity in kitchen area, in %\n",
    "- T2, Temperature in living room area, in Celsius\n",
    "- RH_2, Humidity in living room area, in %\n",
    "- T3, Temperature in laundry room area\n",
    "- RH_3, Humidity in laundry room area, in %\n",
    "- T4, Temperature in office room, in Celsius\n",
    "- RH_4, Humidity in office room, in %\n",
    "- T5, Temperature in bathroom, in Celsius\n",
    "- RH_5, Humidity in bathroom, in %\n",
    "- T6, Temperature outside the building (north side), in Celsius\n",
    "- RH_6, Humidity outside the building (north side), in %\n",
    "- T7, Temperature in ironing room , in Celsius\n",
    "- RH_7, Humidity in ironing room, in %\n",
    "- T8, Temperature in teenager room 2, in Celsius\n",
    "- RH_8, Humidity in teenager room 2, in %\n",
    "- T9, Temperature in parents room, in Celsius\n",
    "- RH_9, Humidity in parents room, in %\n",
    "- To, Temperature outside (from Chievres weather station), in Celsius\n",
    "- Pressure (from Chievres weather station), in mm Hg\n",
    "- RH_out, Humidity outside (from Chievres weather station), in %\n",
    "- Wind speed (from Chievres weather station), in m/s\n",
    "- Visibility (from Chievres weather station), in km\n",
    "- Tdewpoint (from Chievres weather station), in C degrees\n",
    "\n",
    "You are given two datasets: energy_appliances_standard.csv and energy_appliances_extrapolation.csv. The first dataset has typical consumption patterns, while the second one has the top 10% highest consumptions and will be used to test the extrapolating capacities of our models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Random Forests (35/100)\n",
    "\n",
    "Random Forests are excellent predictors. Usually we only need to tune one parameter: the number of trees in the model. However, how many trees are enough? Follow these steps:\n",
    "\n",
    "1. Load the training dataset (energy_appliances_standard.csv) and show the descriptive statistics of the variables. (3 pts)\n",
    "\n",
    "2. Create a train / test partition of the data using 30% of the data for the test set and a ```random_state``` value of 20201107(2 pts).\n",
    "\n",
    "3. Follow [this example](https://scikit-learn.org/stable/auto_examples/ensemble/plot_ensemble_oob.html) and train a [Random Forest Regressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html) using ```Appliances``` as the target variable. Test between 50 and 250 trees, plotting the Out-of-Bag (OOB) error after every iteration. Be patient as training can take a while. Written answer: What is the optimal number of trees for your model and why do you think this? (15 pts)\n",
    "\n",
    "4. Train your final random forest with the number of trees you selected in part 3. Apply this model over your test set and over the extrapolating dataset (from the file ```energy_appliances_extrapolation.csv```), calculating the mean absolute percentual error for each dataset.  Show in a scatterplot the predicted value vs the real value of the target variable for both the test set and the extrapolation set (in the same plot), differentiating both by using different colors for the points. Written answer: How does the random forest model perform on predicting Appliance energy usage in the extrapolation data set? If it performs poorly, why? If it performs well, why? *Hint: look at the scatterplot*. (15 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data load\n",
    "energy_consumption=pd.read_csv('energy_appliances_standard.csv')\n",
    "energy_consumption.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train / test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(energy_consumption.iloc[:, 1:], # X\n",
    "                                                    energy_consumption.iloc[:, 0],  # y\n",
    "                                                    test_size = 0.3,           # Size of test\n",
    "                                                    random_state = 20201107)   # Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define random forest\n",
    "energy_forest = RandomForestRegressor(n_estimators=100, # Number doesn't matter as we will change this.\n",
    "                                      criterion='mse',\n",
    "                                      max_depth=None,\n",
    "                                      min_samples_split=2,\n",
    "                                      min_samples_leaf=1,\n",
    "                                      min_weight_fraction_leaf=0.0,\n",
    "                                      max_features='sqrt', # For a proper random forest\n",
    "                                      max_leaf_nodes=None,\n",
    "                                      min_impurity_decrease=0.0,\n",
    "                                      min_impurity_split=None,\n",
    "                                      bootstrap=True,\n",
    "                                      oob_score=True, # Important to calculate the oob score\n",
    "                                      random_state=20210302,\n",
    "                                      verbose=0,\n",
    "                                      warm_start=True # Needs to be incremental\n",
    "                                      )\n",
    "\n",
    "# Range of `n_estimators` values to explore.\n",
    "min_estimators = 50\n",
    "max_estimators = 250\n",
    "\n",
    "# Define error list\n",
    "error_rate = []\n",
    "\n",
    "# iterate over forest\n",
    "for i in range(min_estimators, max_estimators + 1):\n",
    "    energy_forest.set_params(n_estimators=i)\n",
    "    energy_forest.fit(x_train, y_train)\n",
    "\n",
    "    # Record the OOB error for each `n_estimators=i` setting.\n",
    "    oob_error = 1 - energy_forest.oob_score_\n",
    "    error_rate.append((i, oob_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "xs, ys = zip(*error_rate)\n",
    "plt.plot(xs, ys, label='Random Forest Regressor')\n",
    "\n",
    "plt.xlim(min_estimators, max_estimators)\n",
    "plt.xlabel(\"Number of trees\")\n",
    "plt.ylabel(\"OOB error rate\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Written answer: The curve starts going flat at around 225 trees or so. At that point, increasing the number of trees does not lead to an increase in predictive capability OOB as new trees do not bring new knowledge.[3 pts]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final forest\n",
    "energy_forest = RandomForestRegressor(n_estimators=225, # Number doesn't matter as we will change this.\n",
    "                                      criterion='mse',\n",
    "                                      max_depth=None,\n",
    "                                      min_samples_split=2,\n",
    "                                      min_samples_leaf=1,\n",
    "                                      min_weight_fraction_leaf=0.0,\n",
    "                                      max_features='sqrt', # For a proper random forest\n",
    "                                      max_leaf_nodes=None,\n",
    "                                      min_impurity_decrease=0.0,\n",
    "                                      min_impurity_split=None,\n",
    "                                      bootstrap=True,\n",
    "                                      oob_score=False,\n",
    "                                      random_state=20210302,\n",
    "                                      n_jobs=16, # I have 16 cores. Adjust as needed.\n",
    "                                      verbose=0,\n",
    "                                      warm_start=False\n",
    "                                      )\n",
    "\n",
    "energy_forest.fit(x_train, y_train)\n",
    "\n",
    "# Calculate error over test set\n",
    "pred_test = energy_forest.predict(x_test)\n",
    "\n",
    "# Load the second dataset\n",
    "energy_extrapolation = pd.read_csv('energy_appliances_extrapolation.csv')\n",
    "\n",
    "# Calculate the test set\n",
    "pred_extrapolation = energy_forest.predict(energy_extrapolation.iloc[:, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print MAPE over the test set\n",
    "print('Over a random test set, the MAPE of the Random Forest model is %.2f%%' % (mean_absolute_percentage_error(y_test, pred_test)*100))\n",
    "print('Over the extrapolation set, the MAPE of the Random Forest model is %.2f%%' % (mean_absolute_percentage_error(energy_extrapolation['Appliances'], \n",
    "                                                                                                                        pred_extrapolation) *100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the plot\n",
    "pred_data = (pred_test, pred_extrapolation)\n",
    "test_data = (y_test, energy_extrapolation['Appliances'])\n",
    "colors = (\"green\", \"red\")\n",
    "groups = (\"interpolation\", \"extrapolation\")\n",
    "\n",
    "# Create plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "for x, y, color, group in zip(test_data, pred_data, colors, groups):\n",
    "    ax.scatter(x, y, alpha=0.8, c=color, edgecolors='none', s=30, label=group)\n",
    "\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('Real value', fontsize=18)\n",
    "plt.ylabel('Predicted value', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Written answer: While the prediction over the test set is very good, the prediction over the extrapolation set is horrible. Random forests are not able to predict outside the original range! This happens because every tree returns the average of the value of the elements in each leaf, so it is impossible for it to extrapolate. While these models are fairly powerful predictors, they cannot extrapolate at all.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: XGBoosting model (40 pts)\n",
    "\n",
    "Now we'll repeat the process for the XGB model, using an [```XGBRegressor``` object](https://xgboost.readthedocs.io/en/latest/python/python_api.html). The XGBoosting model is much more sensitive to parameter changes though as it allows to tune many different parameters. For this example:\n",
    "\n",
    "1. Written answer: Why do we say we want to use a small learning rate? Why do we say the number of trees to use depends on each dataset/problem? Why do we want to use a small tree depth? (6 pts)\n",
    "\n",
    "2. Selecting a 30% validation sample over the training set, tune your parameters using crossvalidation. Use the following ranges:\n",
    " - Learning rate: [0.01, 0.1, 0.2].\n",
    " - max_depth: 3 to 7.\n",
    " - Number of trees: [350, 400, 450, 500]\n",
    "\n",
    "Leave the other parameters at the values we identified in the lab (except for the objective parameter and those related to classification problems) and use a seed of 20201107. Report the optimal values of your parameters. (20 pts)\n",
    "\n",
    "3. Repeat part 4 of the previous task, but now for your XGB model trained over the optimal parameter combination and the complete training dataset. Plot the variable importance. Written answer: What are the most important variables? Can the XGB model extrapolate? How does it compare to a random forest? (14 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Written answer part 1 (2 pts each):**\n",
    "\n",
    "- **Learning rate: Small values help by not drastically changing the learned functional shape so far. If the LR is large the model learns too much too soon and subsequent trees are not able to extract knowledge efficiently.**\n",
    "\n",
    "- **Number of trees: As the number of trees depends on the complexity of the data and the size of the dataset, we cannot know a priori how many trees are optimal.**\n",
    "\n",
    "- **Tree depth: An XGB model should have small trees to avoid too many strong learners.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the XGB model\n",
    "energy_XGB = XGBRegressor(max_depth=3,                 # Depth of each tree\n",
    "                          learning_rate=0.1,            # How much to shrink error in each subsequent training. Trade-off with no. estimators.\n",
    "                          n_estimators=100,             # How many trees to use, the more the better, but decrease learning rate if many used.\n",
    "                          verbosity=1,                  # If to show more errors or not.\n",
    "                          objective='reg:squarederror',  # Type of target variable.\n",
    "                          booster='gbtree',             # What to boost. Trees in this case.\n",
    "                          gamma=0.001,                  # Minimum loss reduction required to make a further partition on a leaf node of the tree. (Controls growth!)\n",
    "                          subsample=0.632,              # Subsample ratio. Can set lower\n",
    "                          colsample_bytree=1,           # Subsample ratio of columns when constructing each tree.\n",
    "                          colsample_bylevel=1,          # Subsample ratio of columns when constructing each level. 0.33 is similar to random forest.\n",
    "                          colsample_bynode=1,           # Subsample ratio of columns when constructing each split.\n",
    "                          reg_alpha=1,                  # Regularizer for first fit. alpha = 1, lambda = 0 is LASSO.\n",
    "                          reg_lambda=0,                 # Regularizer for first fit.\n",
    "                          random_state=20201107,        # Seed\n",
    "                          missing=None,                 # How are nulls encoded?\n",
    "                          tree_method='hist',        # I have a GPU. This significantly speeds up things. Otherwise use 'hist' and set n_jobs accordingly\n",
    "                          n_jobs=1\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters. Play with this grid!\n",
    "param_grid = dict({'n_estimators': [350, 400, 450, 500],\n",
    "                   'max_depth': [4, 5, 6, 7],\n",
    "                   'learning_rate' : [0.01, 0.1, 0.2]\n",
    "                  })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always a good idea to tune on a reduce sample of the train set, as we will call many functions.\n",
    "val_train = x_train.copy()\n",
    "val_train['y'] = y_train.copy()\n",
    "val_train = val_train.sample(frac = 0.3,                # The fraction to extract\n",
    "                             random_state = 20201107    # The seed.\n",
    "                             )\n",
    "\n",
    "\n",
    "# Define grid search object.\n",
    "GridXGB = GridSearchCV(energy_XGB,        # Original XGB. \n",
    "                       param_grid,          # Parameter grid\n",
    "                       cv = 3,              # Number of cross-validation folds.  \n",
    "                       scoring = 'neg_root_mean_squared_error',   # How to rank outputs.\n",
    "                       n_jobs = 16,         # Parallel jobs. -1 is \"all you have\" Careful, never clash threads. Either paralellize XGB OR this, NOT both.\n",
    "                       refit = False,       # If refit at the end with the best. We'll do it manually.\n",
    "                       verbose = 1          # If to show what it is doing.\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train grid search. This takes a while! Go grab a coffee.\n",
    "GridXGB.fit(val_train.iloc[:, :-1], val_train['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show best params\n",
    "GridXGB.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the XGB model\n",
    "energy_XGB = XGBRegressor(max_depth=GridXGB.best_params_.get('max_depth'),                 # Depth of each tree\n",
    "                          learning_rate=GridXGB.best_params_.get('learning_rate'),            # How much to shrink error in each subsequent training. Trade-off with no. estimators.\n",
    "                          n_estimators=GridXGB.best_params_.get('n_estimators'),             # How many trees to use, the more the better, but decrease learning rate if many used.\n",
    "                          verbosity=1,                  # If to show more errors or not.\n",
    "                          objective='reg:squarederror',  # Type of target variable.\n",
    "                          booster='gbtree',             # What to boost. Trees in this case.\n",
    "                          gamma=0.001,                  # Minimum loss reduction required to make a further partition on a leaf node of the tree. (Controls growth!)\n",
    "                          subsample=0.632,              # Subsample ratio. Can set lower\n",
    "                          colsample_bytree=1,           # Subsample ratio of columns when constructing each tree.\n",
    "                          colsample_bylevel=1,          # Subsample ratio of columns when constructing each level. 0.33 is similar to random forest.\n",
    "                          colsample_bynode=1,           # Subsample ratio of columns when constructing each split.\n",
    "                          reg_alpha=1,                  # Regularizer for first fit. alpha = 1, lambda = 0 is LASSO.\n",
    "                          reg_lambda=0,                 # Regularizer for first fit.\n",
    "                          random_state=20201107,        # Seed\n",
    "                          missing=None,                 # How are nulls encoded?\n",
    "                          tree_method='gpu_hist'        # I have a GPU. This significantly speeds up things. Otherwise use 'hist' and set n_jobs accordingly\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit!\n",
    "energy_XGB.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable importance (4 pts)\n",
    "importances = energy_XGB.feature_importances_\n",
    "indices = np.argsort(importances)[::-1] \n",
    "\n",
    "f, ax = plt.subplots(figsize=(3, 8))\n",
    "plt.title(\"Variable Importance - XGB\")\n",
    "sns.set_color_codes(\"pastel\")\n",
    "sns.barplot(y=[x_train.columns[i] for i in indices], \n",
    "            x=importances[indices], \n",
    "            label=\"Total\", color=\"b\")\n",
    "ax.set(ylabel=\"Variable\",\n",
    "       xlabel=\"Variable Importance (Entropy)\")\n",
    "sns.despine(left=True, bottom=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Written answer: We can see the most important variables are temperature in the living room and the lights energy consumption, hinting at night time use of the house, while the less important ones are the temperature in the kitchen and the visibility outside.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now the answer is equivalent to T1.\n",
    "pred_test = energy_XGB.predict(x_test)\n",
    "\n",
    "# Calculate the test set\n",
    "pred_extrapolation = energy_XGB.predict(energy_extrapolation.iloc[:, 1:])\n",
    "\n",
    "# Print MAPE over the test set\n",
    "print('Over a random test set, the MAPE of the XGB model is %.2f%%' % (mean_absolute_percentage_error(y_test, pred_test)*100))\n",
    "print('Over the extrapolation set, the MAPE of the XGB model is %.2f%%' % (mean_absolute_percentage_error(energy_extrapolation['Appliances'], \n",
    "                                                                                                                        pred_extrapolation) *100))\n",
    "# Plot\n",
    "# Make the plot\n",
    "pred_data = (pred_test, pred_extrapolation)\n",
    "test_data = (y_test, energy_extrapolation['Appliances'])\n",
    "colors = (\"green\", \"red\")\n",
    "groups = (\"interpolation\", \"extrapolation\")\n",
    "\n",
    "# Create plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "for x, y, color, group in zip(test_data, pred_data, colors, groups):\n",
    "    ax.scatter(x, y, alpha=0.8, c=color, edgecolors='none', s=30, label=group)\n",
    "\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('Real value', fontsize=18)\n",
    "plt.ylabel('Predicted value', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Written answer: The XGB model has a slightly worse performance than the Random Forest, and it is also a very bad extrapolator. The XGB model is also based on trees so there is no way it can extrapolate.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Combined model (25 pts)\n",
    "\n",
    "Now we will finally train a model on the combined data, by joining the extrapolation and the original dataset, and study the performance over the original test set, the new test set and the combined result. For this we will only use the Random Forest model. Use a seed of 20201107 for all functions that accept one.\n",
    "\n",
    "1. Create a train / test split set over the extrapolation data, leaving approximately 30% of the data for testing purposes. Combine this train test with the original train set (let's call this the combined train set). (5 pts)\n",
    "2. Train a Random Forest model over the **combined** train data. Discuss how many trees you used and why. (15 pts)\n",
    "3. Plot the variable importance and compare it versus the XGB in task 1. Now that you more data, does the importance change? (5 pts)\n",
    "3. Report the test set performance of your new model over the original test set, the test set you took over the extrapolation dataset and the combined test set. Plot the scatterplot of the both datasets as before in the same plot, differentiating the dataset by using colours. (10 pts)\n",
    "\n",
    "Written answer: What happens now? What can you say about the new model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train / test split of the extrapolation data [2 pts]\n",
    "x_train_extra, x_test_extra, y_train_extra, y_test_extra = train_test_split(energy_extrapolation.iloc[:, 1:], # X\n",
    "                                                                            energy_extrapolation.iloc[:, 0],  # y\n",
    "                                                                            test_size = 0.3,           # Size of test\n",
    "                                                                            random_state = 20201107)   # Seed\n",
    "\n",
    "# Combined sets [3 pts]\n",
    "x_train_full = pd.concat([x_train, x_train_extra], axis = 0, ignore_index = True)\n",
    "y_train_full = pd.concat([y_train, y_train_extra], axis = 0, ignore_index = True)\n",
    "x_test_full = pd.concat([x_test, x_test_extra], axis = 0, ignore_index = True)\n",
    "y_test_full = pd.concat([y_test, y_test_extra], axis = 0, ignore_index = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To get the optimal number of trees I will start from 200 trees. It does not make much sense to have less if I have more and more complex data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define random forest\n",
    "energy_forest = RandomForestRegressor(n_estimators=100, # Number doesn't matter as we will change this.\n",
    "                                      criterion='mse',\n",
    "                                      max_depth=None,\n",
    "                                      min_samples_split=2,\n",
    "                                      min_samples_leaf=1,\n",
    "                                      min_weight_fraction_leaf=0.0,\n",
    "                                      max_features='sqrt', # For a proper random forest\n",
    "                                      max_leaf_nodes=None,\n",
    "                                      min_impurity_decrease=0.0,\n",
    "                                      min_impurity_split=None,\n",
    "                                      bootstrap=True,\n",
    "                                      oob_score=True, # Important to calculate the oob score\n",
    "                                      random_state=20210302,\n",
    "                                      verbose=0,\n",
    "                                      warm_start=True # Needs to be incremental\n",
    "                                      )\n",
    "\n",
    "# Range of `n_estimators` values to explore.\n",
    "min_estimators = 200\n",
    "\n",
    "max_estimators = 400\n",
    "\n",
    "# Define error list\n",
    "error_rate = []\n",
    "\n",
    "# iterate over forest\n",
    "for i in range(min_estimators, max_estimators + 1):\n",
    "    energy_forest.set_params(n_estimators=i)\n",
    "    energy_forest.fit(x_train_full, y_train_full)\n",
    "\n",
    "    # Record the OOB error for each `n_estimators=i` setting.\n",
    "    oob_error = 1 - energy_forest.oob_score_\n",
    "    error_rate.append((i, oob_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "xs, ys = zip(*error_rate)\n",
    "plt.plot(xs, ys, label='Random Forest Regressor')\n",
    "\n",
    "plt.xlim(min_estimators, max_estimators)\n",
    "plt.xlabel(\"Number of trees\")\n",
    "plt.ylabel(\"OOB error rate\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**After 330 trees the model stabilizes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final forest\n",
    "energy_forest = RandomForestRegressor(n_estimators=330, # Final number of trees\n",
    "                                      criterion='mse',\n",
    "                                      max_depth=None,\n",
    "                                      min_samples_split=2,\n",
    "                                      min_samples_leaf=1,\n",
    "                                      min_weight_fraction_leaf=0.0,\n",
    "                                      max_features='sqrt', # For a proper random forest\n",
    "                                      max_leaf_nodes=None,\n",
    "                                      min_impurity_decrease=0.0,\n",
    "                                      min_impurity_split=None,\n",
    "                                      bootstrap=True,\n",
    "                                      oob_score=False,\n",
    "                                      random_state=20210302,\n",
    "                                      n_jobs=16, # I have 16 cores. Adjust as needed.\n",
    "                                      verbose=0,\n",
    "                                      warm_start=False\n",
    "                                      )\n",
    "\n",
    "energy_forest.fit(x_train_full, y_train_full)\n",
    "\n",
    "# Calculate error over test set\n",
    "pred_test_full = energy_forest.predict(x_test_full)\n",
    "\n",
    "# Calculate error over test set\n",
    "pred_test = energy_forest.predict(x_test)\n",
    "\n",
    "# Calculate the test set\n",
    "pred_extrapolation = energy_forest.predict(energy_extrapolation.iloc[:, 1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10 pts for tuning the number of trees and 5 pts for the forest itself.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable importance (3 pts)\n",
    "importances = energy_forest.feature_importances_\n",
    "indices = np.argsort(importances)[::-1] \n",
    "\n",
    "f, ax = plt.subplots(figsize=(3, 8))\n",
    "plt.title(\"Variable Importance - XGB\")\n",
    "sns.set_color_codes(\"pastel\")\n",
    "sns.barplot(y=[x_train.columns[i] for i in indices], \n",
    "            x=importances[indices], \n",
    "            label=\"Total\", color=\"b\")\n",
    "ax.set(ylabel=\"Variable\",\n",
    "       xlabel=\"Variable Importance (Entropy)\")\n",
    "sns.despine(left=True, bottom=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Written answer: We can see the humidities now are more imporant than the temperatures. On the other hand, the visibility still is the less important variable. [2 pts]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print MAPE over the test set\n",
    "print('Over a random test set, the MAPE of the Random Forest model is %.2f%%' % (mean_absolute_percentage_error(y_test_full, pred_test_full)*100))\n",
    "print('Over a random test set (non-extrapolation), the MAPE of the Random Forest model is %.2f%%' % (mean_absolute_percentage_error(y_test, pred_test)*100))\n",
    "print('Over the extrapolation set, the MAPE of the Random Forest model is %.2f%%' % (mean_absolute_percentage_error(energy_extrapolation['Appliances'], \n",
    "                                                                                                                        pred_extrapolation) *100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the plot\n",
    "pred_data = (pred_test, pred_extrapolation)\n",
    "test_data = (y_test, energy_extrapolation['Appliances'])\n",
    "colors = (\"green\", \"red\")\n",
    "groups = (\"interpolation\", \"extrapolation\")\n",
    "\n",
    "# Create plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "for x, y, color, group in zip(test_data, pred_data, colors, groups):\n",
    "    ax.scatter(x, y, alpha=0.8, c=color, edgecolors='none', s=30, label=group)\n",
    "\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('Real value', fontsize=18)\n",
    "plt.ylabel('Predicted value', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Written answer: This is the importance of having a complete dataset, particularly for tree-based models. The combined model has a lower predictive capability over the original dataset (21% versus 32% MAPE), but it has an excellent predictive capability over the extrapolation test set. When we add the extra data the original patterns are not as clear as before, thus the model losses some predictive capability in that segment. However, it is able to generate a much better prediction over the formerly extrapolated dataset, with an impressive MAPE of around 20%.**\n",
    "\n",
    "Always understand the limitations of your datasets! You won't be able to create models that are always good for every situation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
