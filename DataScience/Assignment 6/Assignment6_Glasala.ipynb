{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grade: /100 pts\n",
    "\n",
    "# Assignment 06: Feature Selection and Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn as sk\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# You're Still a Data Scientist!\n",
    "\n",
    "Your models from the last assignment really impressed some in the management in your football club. Now that you have learned the art of regularization, your boss thinks you should do equally well with much less data. This will save a lot of money the next time around. This time you only get a data set with 3000 observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Preprocessing (5 pts)\n",
    "Tasks:\n",
    "* Load the data present in 'footballer_small.csv' using the pandas library and store the loaded data in a dataframe\n",
    "* Drop the variables: 'ID','club','club_logo','flag', 'nationality','photo','potential', 'birth_date'\n",
    "* Dummy code the variables: work_rate_att, work_rate_def, preferred_foot. **Because we are running a regularized model, we do not want to drop the first column**. If you want to understand why this is, have a look at the Jupyter notebook (`Ridge_And_Dummycoding.ipynb`).  \n",
    "* Get a test data set of size 500 - to make results comparable to solutions, set random_state = 0\n",
    "* visualize all variables of the first 50 observations of the Training data set as an image (see Lab06_followalong). You can also look at it as a data frame. How are the different variables scaled? Which variables have high and which ones have low values?    \n",
    "\n",
    "To make sure that you get a good start - check the solutions from Assignment 4. And make sure you can apply these steps flexibly and quickly. **You will need it for the midterm!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('footballer_small.csv')\n",
    "\n",
    "# Drop the aformentioned columns\n",
    "model_data = df.drop(['ID','club','club_logo','flag', 'nationality','photo','potential', 'birth_date'], axis = 'columns')\n",
    "\n",
    "# In order to get dummies, convert categorical data to categorical type\n",
    "model_data['work_rate_att'] = pd.Categorical(model_data.work_rate_att, categories=['Low','Medium','High'])\n",
    "model_data['work_rate_def'] = pd.Categorical(model_data.work_rate_def, categories=['Low','Medium','High'])\n",
    "model_data['preferred_foot'] = pd.Categorical(model_data.preferred_foot, categories = ['Left','Right'])\n",
    "\n",
    "# Dummies, dropping the first category\n",
    "model_data = pd.get_dummies(model_data, drop_first=False)\n",
    "\n",
    "y = model_data.overall\n",
    "X = model_data.drop('overall', axis = 'columns')\n",
    "\n",
    "# Random state assures that folds are consistent across models\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X,\n",
    "                                                y, \n",
    "                                                test_size = 500, \n",
    "                                                random_state = 0)\n",
    "print('Training set size:',Xtrain.shape)\n",
    "\n",
    "plt.imshow(Xtrain[:50])\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your written answer here**\n",
    "0-9 and 12-39 have high values , the rest low"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Standardization  (10 pts)\n",
    "When using regularized regression models, the scaling of the different regressors can influence the results dramatically (see lectures). One simple solution is to standardize all features before estimating the model, so that no feature can dominate others due to differences in feature scales. \n",
    "\n",
    "a) Use the sklearn class `StandardScaler` to produce a z-scale version of your training data set. Again visualize the the first 50 observations an image. Compare to the plot that you got in Question 1. What do you observe? \n",
    "\n",
    "b) Plot a histogram of the second column (height_cm) of the non-standarized and standardized training set. What is the mean and variance of the standardized training set. \n",
    "\n",
    "c) Build a model `pipeline` that first standarizes all the features in the training set and then fits a `LinearRegression` model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "transformed_X = scaler.fit_transform(Xtrain)\n",
    "plt.imshow(transformed_X[:50,])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Written answer here** Greater variation within variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b) Plot historgram \n",
    "plt.figure(figsize=(15,6))\n",
    "plt.subplot(1, 2, 1)\n",
    "ax = sns.distplot(Xtrain.stamina,\n",
    "                 bins=50,\n",
    "                 kde=True,\n",
    "                 color='skyblue',\n",
    "                 hist_kws={\"linewidth\": 15,'alpha':1})\n",
    "ax.set(xlabel='stamina', ylabel='frequency', title=\"Before Standardization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Written answer here**\n",
    "After standardization, the data is more centered around 0 representing the mean and concentrated within one std from the mean being the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c: Build pipeline\n",
    "model_pipeline = Pipeline([\n",
    "    ('standardize', StandardScaler()),\n",
    "    ('reg', sk.linear_model.LinearRegression())\n",
    "])\n",
    "standardizer_step = model_pipeline.named_steps['standardize']\n",
    "transformed_X = standardizer_step.fit_transform(Xtrain)\n",
    "plt.subplot(1, 2, 2)\n",
    "ax = sns.distplot(transformed_X[:, Xtrain.columns.get_loc(\"stamina\")],\n",
    "                 bins=50,\n",
    "                 kde=True,\n",
    "                 color='skyblue',\n",
    "                 hist_kws={\"linewidth\": 15,'alpha':1})\n",
    "ax.set(xlabel='stamina', ylabel='frequency', title=\"After Standardization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: Comparing different complex features (10 pts)\n",
    "In this task, we will first consider a model that includes all the variables in the data AND all quadratic terms (i.e. each features to the power of two, and the products (interactions) between all possible pairs of features. \n",
    "\n",
    "a) Generate a design matrix for the model. You can use sklearn's `PolynomialFeatures` to do the job. Because sklearn's linear models have the option to fit the intercept, internally, set the `include_bias` option to `False`. \n",
    "* How many linear terms are in each of the new feature set?\n",
    "* How many squared terms are in each of the new feature set?\n",
    "* How many interaction terms are in each of the new feature set? Give an example of one of the interaction terms. \n",
    "\n",
    "b) Now generate 3 more models / design matrices. Again it should include all quadratic terms and 2-way interactions - but each model should drop one of the features\n",
    "* Second Model: Drop `standing_tackle`\n",
    "* Third Model: Drop `composure`\n",
    "* Fourth model: Drop `marking`\n",
    "\n",
    "Hint: For these models, create the design matrix without the aforementioned features and then apply polynomial expansion to the remaing features. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a) Make the new expanded design matrix \n",
    "poly = sk.preprocessing.PolynomialFeatures(2,include_bias=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Written answer here**\n",
    "\n",
    " 47/48 linear linear terms\n",
    "\n",
    " 48x47/2  (49x48/2) interaction terms including includes the squared terms. *Computed using trianglular number*\n",
    "\n",
    " Total of 1128 (1176) terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make design matrices without one of the features\n",
    "Xwithout_standing = Xtrain.drop(['standing_tackle'], axis='columns')\n",
    "Xwithout_standing = poly.fit_transform(Xwithout_standing)\n",
    "Xwithout_composure = Xtrain.drop(['composure'], axis='columns')\n",
    "Xwithout_composure = poly.fit_transform(Xwithout_composure)\n",
    "Xwithout_marking= Xtrain.drop(['marking'], axis='columns')\n",
    "Xwithout_marking = poly.fit_transform(Xwithout_marking)\n",
    "X_new_train = poly.fit_transform(Xtrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4: Evaluating the backward feature search (15 pts)\n",
    "In this question, you have to use the pipeline created in question 2 and apply it to each of the models in question 3. Use 10-fold cross validation to report the validation error on the models using mean squared error as the metric. <br>\n",
    "Show all the steps of the process and compare and analyze the results using the validation error reported. For this first step in the backwards search, which feature would you drop? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "cv_score=cross_val_score(model_pipeline, X_new_train, ytrain, cv=10, scoring='neg_mean_squared_error')\n",
    "print('base model', -cv_score.mean())\n",
    "\n",
    "cv_score=cross_val_score(model_pipeline, Xwithout_standing, ytrain, cv=10, scoring='neg_mean_squared_error')\n",
    "print('model without standing tackle', -cv_score.mean())\n",
    "\n",
    "\n",
    "cv_score=cross_val_score(model_pipeline, Xwithout_composure, ytrain, cv=10, scoring='neg_mean_squared_error')\n",
    "print('model without composure', -cv_score.mean())\n",
    "\n",
    "cv_score=cross_val_score(model_pipeline, Xwithout_marking, ytrain, cv=10, scoring='neg_mean_squared_error')\n",
    "print('model without marking', -cv_score.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Written answer here** \n",
    "I would drop the base model that includes all the features as it is the worst performing model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5: Applying Ridge Regression (10 pts)\n",
    "Build a pipeline that performs scaling and fits the ridge regression on the data that includes the polynomial expansion of all the features. The ridge parameter ($\\lambda$ or `alpha` in sklearn) should be set to 0.5. Use the pipeline to report the validation error using mean square error metric. Use 10-fold cross validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "pip = Pipeline([\n",
    "    ('standardize', StandardScaler()),\n",
    "    ('reg', Ridge(alpha=0.5, fit_intercept=True))\n",
    "])\n",
    "cv_scores = cross_val_score(pip,\n",
    "                           X_new_train,\n",
    "                           ytrain,\n",
    "                           cv=10,\n",
    "                           scoring='neg_mean_squared_error')\n",
    "meancv=-cv_scores.mean()\n",
    "print(meancv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6: Tune the Ridge coefficient for the 2nd-order model (15 pts)\n",
    "Perform the search going from $\\lambda = \\exp(-8), \\cdots, \\exp(6)$ in 15 evenly spaced increments on the log scale. \n",
    "\n",
    "For each setting of lambda, calculate the training error when fitting the regularized model to the entire trainign data set, and the prediction error by studying the performance on the left-out part using 10-fold cross-validation. (*Note this calculation can take a bit, be patient*)\n",
    "\n",
    "Plot the mean squared training error and mean squared validation error as a function of $\\log(\\lambda)$. \n",
    "\n",
    "Note: Although you can ultimately use `GridSearchCV` from sklearn, in this task you need to program a for-loop interating over all the levels of $\\lambda$.  \n",
    "\n",
    "### Questions: \n",
    "\n",
    "What is the best regularization parameter? \n",
    "\n",
    "Why does the validation error increase as $\\lambda \\rightarrow 0$, while the training error decreases from the optimal value?  Why does both the training and the validation error increase when $\\lambda \\rightarrow \\infty$?  Answer in terms of the bias variance trade off and model complexity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "params = {'reg__alpha': np.exp(np.linspace(-8,6,15))}\n",
    "gscv = GridSearchCV(pip, param_grid=params, cv=10, scoring = 'neg_mean_squared_error', refit=True)\n",
    "gscv.fit(X_new_train, ytrain)\n",
    "\n",
    "results = pd.DataFrame(gscv.cv_results_)\n",
    "\n",
    "plt.scatter( np.linspace(-8, 6,15), -results.mean_test_score)\n",
    "plt.xlabel(r'$\\log(\\lambda)$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Written answer here**\n",
    "\n",
    "The best reg parameter can be found using `gscv.best_params_`. \n",
    "\n",
    "As $\\lambda \\rightarrow \\infty$, the model becomes increasingly biased increasingt the RMSE.  RMSE does not increase as $\\lambda \\rightarrow 0$ because it has a low variance high bias model which is a special case of linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7: Test error  (10 pts)\n",
    "Now fit the model using ridge regression, using the lambda-value that you determined works best (in terms of crossvalidated mse) from Question 6. Fit the model on the whole training set. \n",
    "\n",
    "Report the mean squared error on the test data - along with the 95% confidence interval, determined with the central limit theorem (remember assignment 5). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "X_new_test = poly.fit_transform(Xtest)\n",
    "ypred = gscv.predict(X_new_test)\n",
    "sqerr = (ytest-ypred)**2 \n",
    "test_error = sqerr.mean() \n",
    "test_ci = test_error + 1.96 * np.std(sqerr) / np.sqrt(len(sqerr)) * np.array([-1, 1])\n",
    "\n",
    "print(\"Mean test error: \",test_error)\n",
    "print(\"95% CI: \",test_ci)\n",
    "\n",
    "ss = (ytest-ytest.mean())**2 \n",
    "print(\"Proportion variance predicted: \",1-test_error/ss.mean())\n",
    "print(\"95% CI: \",[1-test_ci[1]/ss.mean(), 1-test_ci[0]/ss.mean()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8: Lasso Regression (10 pts)\n",
    "That's great! You can achieve a really good prediction accuracy with much less data than in Week 5. Impressive! \n",
    "\n",
    "Now the problem is that the model is really hard to interpret and explain to clients- the importance of each feature is not easily apparent. So let's build a simpler model, which is only based on the first nine features:\n",
    "* age\n",
    "* height_cm\n",
    "* weight_kg \n",
    "* pac: ???  \n",
    "* sho: shooting \n",
    "* pas: passing \n",
    "* dri: dribble \n",
    "* def: defense\n",
    "* phy: Physiological VO2 max\n",
    "\n",
    "Build a design matrix using only these nine features. Standardize the design matrix using the standard scalar. \n",
    "Then use `sklearn.linear_model.lasso_path` to create a plot of the regression coefficients against the log-regularization parameter (see `Lab06_followalong`). Note that it is standard practice to plot on the x-axis the negative log-lambda values, such that the high regularization (and hence the simpler models) are on the left. \n",
    "\n",
    "Which of the 9 variables drops out of the predicitive model first? Which one is retained for the longest time? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a new, reduced design matrix \n",
    "# Aformentioned columns\n",
    "x = df[['age','height_cm','weight_kg','pac','sho','pas','dri','def','phy']]\n",
    "\n",
    "#Standardize, unsure how to standardize X properly as when it is fit the coef are 0, \n",
    "# and would take an unreasonable amount of time to run ?\n",
    "\n",
    "poly = sk.preprocessing.PolynomialFeatures(6)\n",
    "X = poly.fit_transform(x)\n",
    "scaler = sk.preprocessing.StandardScaler(with_mean=True, with_std=True)\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Create a lasso path\n",
    "eps = 5e-3 # The smaller eps, the longer the path  \n",
    "lambda_lasso, coefs_lasso, _ = sk.linear_model.lasso_path(X, y, eps, n_alphas=100, alphas= None, fit_intercept=False)\n",
    "\n",
    "print(f\"minmum regularization parameter : {np.amin(lambda_lasso)}\")\n",
    "print(f\"maximum regularization parameter : {np.amax(lambda_lasso)}\")\n",
    "\n",
    "\n",
    "colors = ['b', 'r', 'g', 'c', 'k','y','purple','orange','pink']\n",
    "neg_log_lambda = -np.log(lambda_lasso)\n",
    "\n",
    "for i in range(9):\n",
    "    l1 = plt.plot(neg_log_lambda, coefs_lasso[i,], c=colors[i])\n",
    "    label = f'Poly:{i}'\n",
    "    plt.annotate(label, # this is the text\n",
    "                 (neg_log_lambda[-1],coefs_lasso[i,-1]), # this is the point to label\n",
    "                 textcoords=\"offset points\", # how to position the text\n",
    "                 xytext=(0,10), # distance from text to points (x,y)\n",
    "                 ha='center',\n",
    "                 color = colors[i]) # horizontal alignment can be left, right or center\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Written answer here**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 9: Tuning the lasso coefficient \n",
    "We now want to find a model that is both simple (explainable), but at the same time yields still relatively decent predictions. To assess this, vary the regularization constant of the lasso model between exp(2) end exp(-3.5). That is, vary negative log lambda between -2 and 3.5 in even steps. \n",
    "Plot the crossvalidation error (mean squared error) against negative-log-lambda of the model. \n",
    "\n",
    "Hint: You can either program a for-loop as in Question 6, or use the function `GridSearchCV`. \n",
    "\n",
    "What is the simplest model that still gives you a expected validation error of below 15? \n",
    "If you wanted the best validation error, what $\\lambda$ would you need to use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# Your code here\n",
    "# Pipeline\n",
    "# pipe2 = Pipeline([('poly', PolynomialFeatures()),\n",
    "                 # ('fit', linear_model.Lasso())])\n",
    "# Params\n",
    "# params = {'reg__alpha': np.exp(np.linspace(math.exp(2),math.exp(-3.5),15))}\n",
    "\n",
    "#'Lasso': GridSearchCV(linear_model.Lasso(), \n",
    "                               # param_grid=lasso_params).fit(df[X], df[Y]).best_estimator_,\n",
    "# for loop that applies exp(2) on even exp(-3.5) on odd\n",
    "\n",
    "# cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# define grid\n",
    "# grid = dict()\n",
    "# grid['alpha'] = arange(0, 1, 0.01)\n",
    "# define search\n",
    "# search = GridSearchCV(model, grid, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "# perform the search\n",
    "# results = search.fit(X, y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Written answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question  10: Lasso vs. Ridge\n",
    "In this quesiton, we will compare Ridge regression and Lasso solutions on the model defined in Question 8. Make sure you are using a standardized design matrix for this task. \n",
    "\n",
    "Fit the model using Ridge (L2- regularization, $\\lambda = exp(-3)$) and Lasso (L1-regularization, $\\lambda = exp(-0.5)$. Then print out the regression coefficients for each of the nine features in the design matrix. \n",
    "\n",
    "Based on the coefficients, which one is the most important feature in the Ridge vs. Lasso solution? How many features are contributing to the prediction for each solution? How can this difference be explained? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Know how to create it but can't run without standardized X\n",
    "\n",
    "#Ridge \n",
    "ridge = sk.linear_model.Ridge(alpha=math.exp(-3),fit_intercept=True)\n",
    "ridge.fit(X,y)\n",
    "\n",
    "#Lasso\n",
    "las = sk.linear_model.Lasso(alpha=math.exp(-0.5),fit_intercept=True)\n",
    "las.fit(X,y)\n",
    "\n",
    "(ridge.coef_,las.coef_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Written answer here**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
